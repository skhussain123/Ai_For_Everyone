{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5490c589",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In the world of machine learning, training models requires a massive amount of computational power and time. When training a neural network to classify an image, such as identifying whether an image is of a cat or not, the process can take weeks or months of continuous computation.\n",
    "\n",
    "Hugging Face is a transformative platform hosting 233,112+ open-source AI models (as of the latest data) across diverse domains like NLP, Computer Vision, and Multimodal tasks. This document provides a structured breakdown of:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9d41f",
   "metadata": {},
   "source": [
    "### 2. Hugging Face Model Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55468b",
   "metadata": {},
   "source": [
    "#### **2.1 Multimodal Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b83bed",
   "metadata": {},
   "source": [
    "Definition: Models that process multiple types of inputs, such as text + images or audio + text.\n",
    "Example: GPT-4 – This model can process both text and images.\n",
    "Use Cases:\n",
    "* Chatbots\n",
    "* Content Generation\n",
    "* Medical Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf6439",
   "metadata": {},
   "source": [
    "#### **2.2 Computer Vision (CV)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bd6dd",
   "metadata": {},
   "source": [
    "**Key Tasks:**\n",
    "* Image Classification: Assigning labels to images (e.g., \"cat\" or \"dog\").\n",
    "  * Popular Model: ResNet (4.27M+ downloads).\n",
    "* Object Detection: Identifying objects in images (e.g., Snapchat filters detecting faces).\n",
    "* Image Segmentation: Segmenting an image at the pixel level (e.g., isolating a cat in an image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f465302",
   "metadata": {},
   "source": [
    "#### **2.3 Natural Language Processing (NLP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa276a6",
   "metadata": {},
   "source": [
    "* Text Classification: Categorizing text based on sentiment or other features.\n",
    "Models Available: 25,656+ models for text classification.\n",
    "* Question Answering: Extracting answers from documents.\n",
    "  * Models Available: 5,218+ models.\n",
    "* Named Entity Recognition (NER): Identifying entities like \"Ahmed\" (person) or \"Pakistan\" (location).\n",
    "* Zero-Shot Classification: Predicting labels without prior training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f0c63",
   "metadata": {},
   "source": [
    "#### **2.4 Audio & Other Domains**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc57c5",
   "metadata": {},
   "source": [
    "1. Audio Models:\n",
    "* Speech Recognition: Converting speech into text.\n",
    "* Music Generation: Creating music from prompts.\n",
    "2. Other Domains:\n",
    "* Tabular: Models for structured data (e.g., spreadsheets).\n",
    "* Reinforcement Learning: Learning based on rewards and penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162681b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbdbecda",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_API_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     image.show()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mgenerate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpink elephant on the road\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgenerate_image\u001b[39m\u001b[34m(prompt_text)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_image\u001b[39m(prompt_text):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdall-e-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"dall-e-2\"\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1024x1024\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# \"512x512\" supported by DALL-E 2\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstandard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     image_url = response.data[\u001b[32m0\u001b[39m].url\n\u001b[32m     18\u001b[39m     image_content = requests.get(image_url).content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Pictures\\ai_for_everyone\\11_Object Recognition, Models_& Parameters_on Hugging_Face\\.venv\\Lib\\site-packages\\openai\\resources\\images.py:322\u001b[39m, in \u001b[36mImages.generate\u001b[39m\u001b[34m(self, prompt, background, model, moderation, n, output_compression, output_format, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    230\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    251\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    252\u001b[39m ) -> ImagesResponse:\n\u001b[32m    253\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m    Creates an image given a prompt.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[33;03m    [Learn more](https://platform.openai.com/docs/guides/images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/images/generations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmoderation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoderation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_compression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstyle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageGenerateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Pictures\\ai_for_everyone\\11_Object Recognition, Models_& Parameters_on Hugging_Face\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Pictures\\ai_for_everyone\\11_Object Recognition, Models_& Parameters_on Hugging_Face\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_API_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "def generate_image(prompt_text):\n",
    "    response = openai.images.generate(\n",
    "        model=\"dall-e-3\",  # or \"dall-e-2\"\n",
    "        prompt=prompt_text,\n",
    "        size=\"1024x1024\",  # \"512x512\" supported by DALL-E 2\n",
    "        quality=\"standard\",\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    image_url = response.data[0].url\n",
    "    image_content = requests.get(image_url).content\n",
    "    image = Image.open(io.BytesIO(image_content))\n",
    "    image.show()\n",
    "\n",
    "# Call the function\n",
    "generate_image('pink elephant on the road')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025b1d6",
   "metadata": {},
   "source": [
    "## What is Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d15796",
   "metadata": {},
   "source": [
    "AI ya machine learning mein pipeline ka matlab hota hai ek aisi sequence ya steps ki line jo data ko process karne se le kar model banane aur usay deploy karne tak poora kaam asaani se aur systematic tareeqe se karti hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7480afa",
   "metadata": {},
   "source": [
    " ### AI Pipeline ke Mukhtasir Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4260e",
   "metadata": {},
   "source": [
    "**1. Data Collection**\n",
    "   * Data ikattha karna (web se, sensors, files, ya database se).\n",
    "\n",
    "**2. Data Preprocessing**\n",
    "   * Data ko saaf aur tayyar karna (missing values ko theek karna, numbers mein convert karna, etc.).\n",
    "\n",
    "**3. Feature Engineering**\n",
    "   * Aise features banana jo model ko achi tarah train karne mein madad dein.\n",
    "\n",
    "**3. Model Training**\n",
    "   * Algorithm (jaise Decision Tree ya Neural Network) use karke model train karna.\n",
    "\n",
    "**4. Hyperparameter Tuning**\n",
    "   * Model ke settings ko optimize karna taake behtar results milen.\n",
    "\n",
    "**5. Model Deployment**\n",
    "   * Model ko real-life application ya website mein use karne ke liye deploy karna.\n",
    "\n",
    "**6. Monitoring**\n",
    "   * Model ka performance regularly check karna aur update karna agar zarurat ho.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27723d18",
   "metadata": {},
   "source": [
    "### ✅ Pipeline Use Karne ke Fayde:\n",
    "\n",
    "1. Automation – Har dafa manually kaam karne ki zarurat nahi.\n",
    "2. Reusability – Ek hi pipeline bar bar use ki ja sakti hai.\n",
    "3. Consistency – Har step har dafa same tareeqe se hota hai.\n",
    "4. Asaan Deployment – Code ko production mein le jana simple ho jata hai."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
